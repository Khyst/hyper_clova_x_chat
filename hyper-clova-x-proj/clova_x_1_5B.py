import torch
import time
import os
import re # ì •ê·œ í‘œí˜„ì‹ ëª¨ë“ˆ
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# LangChain RAG ë° Core ëª¨ë“ˆ ì„í¬íŠ¸
from langchain_core.prompts import PromptTemplate 
from langchain_community.llms import HuggingFacePipeline
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from operator import itemgetter

# --- ì¶œë ¥ í´ë¦¬ë‹ í•¨ìˆ˜ ìµœì¢… ê°•í™” ---
def clean_llm_response(text):
    """
    LLM ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ íƒœê·¸, ë°˜ë³µ, ì½”ë“œ ë¸”ë¡ì„ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
    """
    text = text.strip()
    
    # 1. ë°˜ë³µì ì¸ Chat Template íƒœê·¸ ë° ë¶ˆí•„ìš”í•œ ì‹œì‘ í‚¤ì›Œë“œ ì •ì˜
    # ì‚¬ìš©ìê°€ ë³´ê³ í•œ íŠ¹ì • ë°˜ë³µ ë¬¸êµ¬ í¬í•¨
    keywords_to_remove = [
        "ë‹µë³€:", "assistant:", "<|im_start|>assistant", "user", "system",
        "ì´ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” ë³µì¡í•œ ì‘ì—…ì„ ë” ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.assistant",
        "LangChainì€ ì–¸ì–´ ëª¨ë¸ ì²´ì¸ì„ í†µí•´ ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ê³¼ ë„êµ¬ë¥¼ í†µí•©í•˜ì—¬ ì‘ì—… ìˆ˜í–‰ì„ ìë™í™”í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."
    ]
    
    # ìµœëŒ€ 5ë²ˆ ë°˜ë³µ ì œê±° ì‹œë„ (ë” ê³µê²©ì ìœ¼ë¡œ)
    for _ in range(5): 
        original_text = text
        
        # í‚¤ì›Œë“œ ì œê±°
        for keyword in keywords_to_remove:
            if text.startswith(keyword):
                text = text[len(keyword):].strip()
        
        # íŠ¹ì • ë°˜ë³µ íŒ¨í„´ ì œê±° (ë¬¸ì¥ ìì²´ ì œê±°)
        if text.startswith("ì´ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” ë³µì¡í•œ ì‘ì—…ì„ ë” ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."):
             text = text[len("ì´ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” ë³µì¡í•œ ì‘ì—…ì„ ë” ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."):].strip()
        
        if text == original_text:
            break

    # 2. ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì½”ë“œ ë¸”ë¡ (````...````) ë° ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì œê±°
    # re.DOTALL í”Œë˜ê·¸ëŠ” .ì´ ì¤„ ë°”ê¿ˆ ë¬¸ìë„ í¬í•¨í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    text = re.sub(r'```.*?```', '', text, flags=re.DOTALL).strip()
    text = re.sub(r'```markdown.*?$', '', text, flags=re.DOTALL).strip() # ë§ˆí¬ë‹¤ìš´ ì‹œì‘ íƒœê·¸ ë° ê·¸ ì´í›„ í…ìŠ¤íŠ¸ ì œê±°
    
    # 3. ë¶ˆí•„ìš”í•œ ë¹ˆ ì¤„ ì œê±° ë° ì¤„ ë°”ê¿ˆ ì •ë¦¬
    text_lines = [line.strip() for line in text.splitlines() if line.strip()]
    text = '\n'.join(text_lines)
    
    return text.strip()


# --- í™˜ê²½ ì„¤ì • ë° ê°€ìƒ ë¬¸ì„œ íŒŒì¼ ìƒì„± ---

RAG_DOC_PATH = "rag_source_document.txt"
if not os.path.exists(RAG_DOC_PATH):
    with open(RAG_DOC_PATH, "w", encoding="utf-8") as f:
        f.write("LangChainì€ LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. "
                "ì£¼ìš” ëª©ì ì€ ì–¸ì–´ ëª¨ë¸ì„ ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ ë° ì»´í“¨íŒ…ê³¼ ì—°ê²°í•˜ì—¬ ê¸°ëŠ¥ì„ í™•ì¥í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. "
                "LangChainì˜ í•µì‹¬ êµ¬ì„± ìš”ì†ŒëŠ” ëª¨ë¸ I/O, ê²€ìƒ‰(Retrieval), ì²´ì¸(Chains) ë“±ì…ë‹ˆë‹¤. "
                "RAGëŠ” ì™¸ë¶€ ì§€ì‹(ë¬¸ì„œ)ì„ ê²€ìƒ‰í•˜ì—¬ LLMì´ ë” ì •í™•í•˜ê³  ìµœì‹  ì •ë³´ê°€ ë°˜ì˜ëœ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤. "
                "\n\nìŠˆë¢°ë”©ê±° ë°©ì •ì‹ì€ ì–‘ìì—­í•™ì˜ ê¸°ë³¸ ë°©ì •ì‹ìœ¼ë¡œ, ì‹œê°„ì— ë”°ë¼ íŒŒë™í•¨ìˆ˜(Wave function, $\Psi$)ê°€ ì–´ë–»ê²Œ ì§„í™”í•˜ëŠ”ì§€ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤. "
                "íŒŒë™í•¨ìˆ˜ì˜ ì ˆëŒ“ê°’ ì œê³±($|\Psi|^2$)ì€ íŠ¹ì • ìœ„ì¹˜ì—ì„œ ì…ìë¥¼ ë°œê²¬í•  í™•ë¥  ë°€ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. "
                "ì–‘ìì—­í•™ì—ì„œ ìŠˆë¢°ë”©ê±° ë°©ì •ì‹ì€ ì´ í™•ë¥ ì  í–‰ë™ì„ ì˜ˆì¸¡í•˜ëŠ” í•µì‹¬ ë„êµ¬ì…ë‹ˆë‹¤.")
    print(f"'{RAG_DOC_PATH}' íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")


# --- 1. ì„¤ì • ë° ë¡œì»¬ ëª¨ë¸ ë¡œë“œ (HuggingFace LLM) ---

local_model_path = "../models/HyperCLOVAX-SEED-Text-Instruct-1.5B" 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\nì‚¬ìš© ì¥ì¹˜: {device}")

try:
    model = AutoModelForCausalLM.from_pretrained(local_model_path, device_map="auto")
    tokenizer = AutoTokenizer.from_pretrained(local_model_path)
    print("LLMì´ ë¡œì»¬ ê²½ë¡œì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# --- 2. Hugging Face Pipeline ë° LLM ê°ì²´ ìƒì„± ---

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=device.index if device.type == "cuda" else -1,
    max_new_tokens=512,
    do_sample=False,
    temperature=0.7,
    return_full_text=False # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸
)

llm = HuggingFacePipeline(pipeline=pipe)
print("HuggingFace LLM íŒŒì´í”„ë¼ì¸ ê°ì²´ ìƒì„± ì™„ë£Œ.")


# --- 3. RAG ì‹œìŠ¤í…œ êµ¬ì„± ---

loader = TextLoader(RAG_DOC_PATH, encoding="utf-8")
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
print(f"ì´ {len(texts)}ê°œì˜ ë¬¸ì„œ ì²­í¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = Chroma.from_documents(texts, embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 2}) 
print("Chroma ë²¡í„°ìŠ¤í† ì–´ ë° Retriever ê°ì²´ ìƒì„± ì™„ë£Œ.")


# --- 4. RAG Chain êµ¬ì„± (LCEL ì‚¬ìš© - ì¶œì²˜ í¬í•¨) ---

RAG_TEMPLATE = """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë§¥ë½(Context)ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(Question)ì— ë‹µë³€í•˜ëŠ” AIì…ë‹ˆë‹¤. 
ì œê³µëœ ë§¥ë½ê³¼ ê´€ë ¨ì´ ì—†ëŠ” ì§ˆë¬¸ì´ë¼ë©´, ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
ë‹µë³€ì€ í•­ìƒ ì‚¬ìš©ëœ ë§¥ë½ì— ê·¼ê±°í•´ì•¼ í•©ë‹ˆë‹¤.

ë§¥ë½(Context):
{context}

ì§ˆë¬¸(Question): {question}

ë‹µë³€:
"""
RAG_PROMPT = PromptTemplate.from_template(RAG_TEMPLATE)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

context_pipe = itemgetter("question") | retriever

answer_pipe = (
    RunnableParallel({
        "context": context_pipe | format_docs, 
        "question": itemgetter("question")
    })
    | RAG_PROMPT
    | llm
)

full_rag_chain = RunnableParallel(
    context=context_pipe, 
    answer=answer_pipe
)
print("RAG Chain íŒŒì´í”„ë¼ì¸ (ë‹µë³€ ë° ì¶œì²˜ í¬í•¨) êµ¬ì¶• ì™„ë£Œ.")


# --- 5. RAG ëª¨ë¸ í…ŒìŠ¤íŠ¸: Chain Invoke ë° ì •ê°ˆí•œ CLI ì¶œë ¥ ---

prompts = [
    "LangChainì€ ì–´ë–¤ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í”„ë ˆì„ì›Œí¬ì•¼?",
    "ìŠˆë¢°ë”©ê±° ë°©ì •ì‹ê³¼ ì–‘ìì—­í•™ì˜ ê´€ê³„ë¥¼ ìµœëŒ€í•œ ìì„¸íˆ ì•Œë ¤ì¤˜.",
    "í”„ë‘ìŠ¤ í˜ëª…ì€ ì–¸ì œ ì¼ì–´ë‚¬ë‹ˆ?",
]

# LLM ì¶œë ¥ì„ ë©ˆì¶œ ì‹œí€€ìŠ¤ ì •ì˜
STOP_SEQUENCES = ["\n\në‹µë³€:", "ë‹µë³€:\n\n", "ë§¥ë½(Context):", "<|endofturn|>", "<|stop|>"]

print("\n" + "=" * 70)
print("RAG Chain í…ŒìŠ¤íŠ¸ ì‹œì‘")
print("=" * 70)

for i, prompt in enumerate(prompts):
    print("\n" + "#" * 70)
    print(f"   ğŸ‘‰ {i+1}. ì‚¬ìš©ì ì§ˆë¬¸: {prompt}")
    print("#" * 70)
    
    start_time = time.time()
    
    # invoke í˜¸ì¶œ ì‹œ configë¥¼ í†µí•´ STOP ì‹œí€€ìŠ¤ ì „ë‹¬
    result = full_rag_chain.invoke(
        {"question": prompt},
        config={
            "stop": STOP_SEQUENCES 
        }
    )

    end_time = time.time()
    elapsed_time = end_time - start_time
    
    # 1. ë‹µë³€ í…ìŠ¤íŠ¸ ì •ë¦¬ ë° í´ë¦¬ë‹ (ê°•í™”ëœ í•¨ìˆ˜ ì‚¬ìš©)
    answer_text = clean_llm_response(result['answer'])
    
    # 2. ì •ê°ˆí•œ ë‹µë³€ ì¶œë ¥
    print("-" * 30 + " ğŸ¤– ë‹µë³€ " + "-" * 30)
    print(answer_text)
    print("-" * 70)
    
    # 3. ì¶œì²˜ ì •ë³´ ì¶œë ¥
    print("ğŸ“š ì‚¬ìš©ëœ ì¶œì²˜ ì •ë³´:")
    
    if result['context']:
        sources = set([doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ') for doc in result['context']])
        for src in sources:
            print(f"   - {src}")
    else:
        print("   - RAG ì²´ì¸ì„ í†µí•´ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    print("-" * 70)
    print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ | ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")


print("\n" + "=" * 70)
print("RAG Chain í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
print("=" * 70)